{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OSPool Site Admin Documentation","text":""},{"location":"#supported-cluster-oses-and-htcondor-versions","title":"Supported Cluster OSes and HTCondor Versions","text":"OS HTCondor Notes EL7 (*) 23.10.* EL7 is no longer supported, and thus our ability to support such systems may be removed at any time. EL8 (*) 24.* (&gt; 24.0) EL9 (*) 24.* (&gt; 24.0) Debian 11 (bullseye) 24.* (&gt; 24.0) Debian 12 (bookworm) 24.* (&gt; 24.0) Ubuntu 20.04 (focal) 24.0.* Ubuntu 20.04 is no longer supported, and thus our ability to support such systems may be removed at any time. Ubuntu 22.04 (jammy) 24.* (&gt; 24.0) Ubuntu 24.04 (noble) 24.* (&gt; 24.0) (*) Tested variants are RHEL, Alma, and Rocky."},{"location":"#monitoring-and-information","title":"Monitoring and Information","text":""},{"location":"#viewing-researcher-jobs-running-within-a-glidein-job","title":"Viewing researcher jobs running within a glidein job","text":"<ol> <li> <p>Log in to a worker node as the OSPool user (e.g., \u201cosg01\u201d but may be different at your site)</p> </li> <li> <p>Pick a glidein (and its directory):</p> <p>Note: SCRATCH is the path of the scratch directory you gave us to put glideins in.</p> <ol> <li> <p>Option 1: List HTCondor processes, pick one, and note its glidein directory:</p> <pre><code>$ ps -u osg01 -f | grep master\nosg01    4122304 [...] SCRATCH/glide_qJDh7z/main/condor/sbin/condor_master [...]\n</code></pre> </li> <li> <p>Option 2: List glidein directories and pick an active one:</p> <pre><code>$ ls -l SCRATCH/glide_*/_GLIDE_LEASE_FILE\n</code></pre> <p>Pick a \u201cglide_xxxxxx\u201d directory that has a lease file with a timestamp less than about 5 minutes old.</p> </li> </ol> <p>Note: Let GLIDEDIR be the path to the chosen glidein directory, e.g., <code>SCRATCH/glide_xxxxxx</code></p> </li> <li> <p>Make sure HTCondor is recent enough:</p> <pre><code>$ GLIDEDIR/main/condor/bin/condor_version\n$CondorVersion: 24.6.1 2025-03-20 BuildID: 794846 PackageID: 24.6.1-1 $\n$CondorPlatform: x86_64_AlmaLinux9 $\n</code></pre> <p>The Condor Version should be 2.7.0 or later; if not, go back to step 2 and pick a different glidein directory.</p> </li> <li> <p>Pick the PID of an HTCondor \u201cstartd\u201d process to query:</p> <ol> <li> <p>If you are just exploring, run the following command and pick any \u201ccondor_startd\u201d process \u2014 it does not matter if it is associated with the HTCondor instance identified in steps 2\u20133 above:</p> <pre><code>$ ps -u osg01 -f | grep condor_startd\n</code></pre> </li> <li> <p>If you are looking for the \u201cstartd\u201d associated with some other process, start with a process tree:</p> <pre><code>$ ps -u osg01 -f --forest\n</code></pre> <p>Find the process of interest, then work upward in the process tree to the nearest ancestor \u201ccondor_startd\u201d process.</p> </li> <li> <p>In either case, note the PID (leftmost number) on the \u201ccondor_startd\u201d line you picked.</p> </li> </ol> </li> <li> <p>Run <code>condor_who</code> on the PID you picked:</p> <pre><code>$ GLIDEDIR/main/condor/bin/condor_who -pid PID -ospool\n&lt;b&gt;Batch System : SLURM&lt;/b&gt;\n&lt;b&gt;Batch Job    : 346675&lt;/b&gt;\n&lt;b&gt;Birthdate    : 2025-04-07 14:47:02&lt;/b&gt;\n&lt;b&gt;Temp Dir     : /var/lib/condor/execute/osg01/glide_qJDh7z/tmp&lt;/b&gt;\n&lt;b&gt;Startd : glidein_4101601_324283152@spark-a220.chtc.wisc.edu has 1 job(s) running:&lt;/b&gt;\nPROJECT   USER   AP_HOSTNAME         JOBID        RUNTIME    MEMORY    DISK      CPUs EFCY PID     STARTER\nInst-Proj jsmith ap20.uc.osg-htc.org 27781234.0   0+00:17:43  512.0 MB  129.0 MB    1 0.00 4124321 4123123\n\u2026\n</code></pre> </li> </ol> <p>Definitions of fields in the header (before the PROJECT USER \u2026 row):</p> Batch System HTCondor\u2019s name for the type of batch system you are running. Batch Job The identifier for this glidein job in your batch system. Birthdate When HTCondor began running within this glidein; typically, this is a few minutes after the glidein job itself began running. Temp Dir The path to the glidein job directory (remove the trailing \u201c/tmp\u201d) Startd HTCondor\u2019s identifier for its \u201cstartd\u201d process within the glidein job. <p>Definitions of fields in each row of the researcher job table:</p> PROJECT The OSPool project identifier for this researcher job USER The OSPool AP\u2019s user identifier for this researcher job AP_HOSTNAME The OSPool AP\u2019s hostname JOBID HTCondor\u2019s identifier for this researcher job on its AP RUNTIME HTCondor\u2019s value for the runtime of the researcher job MEMORY The amount of memory (RAM), in MB, that HTCondor allocated to this researcher job DISK The amount of disk, in KB, that HTCondor allocated to this researcher job CPUs The number of CPU cores that HTCondor allocated to this researcher job EFCY An HTCondor measure of the efficiency of the job, roughly calculated as CPU time / wallclock time; a value noticeably greater than the CPUs value may mean the researcher job is using more cores than requested PID The local PID of the researcher job, or more often, of the root of the process tree for the researcher job (e.g., this could be a wrapper script or even Singularity or Apptainer for a researcher job in a container) STARTER The local PID of the HTCondor \u201cstarter\u201d process that owns this research job"},{"location":"#viewing-researcher-jobs-running-within-all-ospool-glidein-jobs","title":"Viewing researcher jobs running within all OSPool glidein jobs","text":"<p>Note: Steps 1\u20133 are the same as above.</p> <ol> <li> <p>Log in to a worker node as the OSPool user (e.g., \u201cosg01\u201d but may be different at your site)</p> </li> <li> <p>Pick a glidein (and its directory):</p> <p>Note: SCRATCH is the path of the scratch directory you gave us to put glideins in.</p> <ol> <li>Option 1: List HTCondor processes, pick one, and note its glidein directory:</li> </ol> <pre><code>$ ps -u osg01 -f | grep master\nosg01    4122304 [...] SCRATCH/glide_qJDh7z/main/condor/sbin/condor_master [...]\n</code></pre> <ol> <li>Option 2: List glidein directories and pick an active one:</li> </ol> <pre><code>$ ls -l SCRATCH/glide_*/_GLIDE_LEASE_FILE\n</code></pre> <p>Pick a \u201cglide_xxxxxx\u201d directory that has a lease file with a timestamp less than about 5 minutes old.</p> <p>Note: Let GLIDEDIR be the path to the chosen glidein directory, e.g., <code>SCRATCH/glide_xxxxxx</code></p> </li> <li> <p>Make sure HTCondor is recent enough:</p> <pre><code>$ GLIDEDIR/main/condor/bin/condor_version\n$CondorVersion: 24.6.1 2025-03-20 BuildID: 794846 PackageID: 24.6.1-1 $\n$CondorPlatform: x86_64_AlmaLinux9 $\n</code></pre> <p>The Condor Version should be 2.7.0 or later; if not, go back to step 2 and pick a different glidein directory.</p> </li> <li> <p>Run <code>condor_who</code> on all discoverable glideins on this host running as the current user:</p> <pre><code>$ GLIDEDIR/main/condor/bin/condor_who -allpids -ospool\n\nBatch System : SLURM\nBatch Job    : 346675\nBirthdate    : 2025-04-07 14:47:02\nTemp Dir     : /var/lib/condor/execute/osg01/glide_qJDh7z/tmp\nStartd : glidein_4101601_324283152@spark-a220.chtc.wisc.edu has 1 job(s) running:\nPROJECT   USER   AP_HOSTNAME         JOBID        RUNTIME    MEMORY    DISK      CPUs EFCY PID     STARTER\nInst-Proj jsmith ap20.uc.osg-htc.org 27781234.0   0+00:17:43  512.0 MB  129.0 MB    1 0.00 4124321 4123123\n\u2026\n</code></pre> </li> </ol> <p>For each glidein job, there will be one set of heading lines (\u201cBatch System\u201d, etc.) and a table of researcher jobs, one per line; format and definitions are as above.</p> <p>OSPool Contribution Requirements</p>"},{"location":"#contributing-via-a-hosted-ce","title":"Contributing via a Hosted CE","text":"<ul> <li> <p>The cluster and login node are set up for our user account:</p> </li> <li> <p>The cluster is operational and generally works</p> </li> <li> <p>The user account has a home directory on the login node</p> </li> <li> <p>The user account can read, write, and execute files and directories within its home directory</p> </li> <li> <p>Our home directory has enough available space and inodes (TBD but not a lot)</p> </li> <li> <p>PATH staff know the right partition (and other batch system config) to use</p> </li> <li> <p>The batch system is configured to allow the user account to submit jobs to the right partition(s) and for the default job \u201cshape\u201d (e.g., 1 core, 2 GB memory, and 24-hour maximum run time)</p> </li> <li> <p>It is possible to SSH from the CE to the login node:</p> </li> <li> <p>PATh staff know the current hostname of your login node</p> </li> <li> <p>That hostname has a public DNS entry that resolves to the correct IP address</p> </li> <li> <p>PATh staff know the user account name (default, \u201cosg01\u201d)</p> </li> <li> <p>PATh staff know about SSH configuration details to use (e.g., alternate port, jump host)</p> </li> <li> <p>The SSH client on one of our IP addresses can connect to your login node (through firewalls, etc.)</p> </li> <li> <p>The provided SSH public key has been installed in the right place and with the right permissions</p> </li> <li> <p>The provided SSH public key is sufficient for authentication by your SSH server</p> </li> <li> <p>The worker nodes on which our jobs may run are ready:</p> </li> <li> <p>Our home directory is shared with each cluster node</p> </li> <li> <p>PATh staff know the correct path to scratch space for jobs (ideally on each worker node, but a shared filesystem may work)</p> </li> <li> <p>Our user account can create subdirectories and run executables in the scratch directory</p> </li> <li> <p>The worker nodes have permissive outbound network connectivity to the Internet (default allow, please note specific restrictions)</p> </li> </ul>"},{"location":"ospool-requirements/","title":"OSPool Contribution Requirements","text":""},{"location":"ospool-requirements/#contributing-via-a-hosted-ce","title":"Contributing via a Hosted CE","text":""},{"location":"ospool-requirements/#the-cluster-and-login-node-are-set-up-for-our-user-account","title":"The cluster and login node are set up for our user account","text":"<ul> <li>The cluster is operational and generally works</li> <li>The user account has a home directory on the login node</li> <li>The user account can read, write, and execute files and directories within its home directory</li> <li>Our home directory has enough available space and inodes (TBD but not a lot)</li> <li>PATH staff know the right partition (and other batch system config) to use</li> <li>The batch system is configured to allow the user account to submit jobs to the right partition(s) and for the default job \u201cshape\u201d (e.g., 1 core, 2 GB memory, and 24-hour maximum run time)</li> </ul>"},{"location":"ospool-requirements/#it-is-possible-to-ssh-from-the-ce-to-the-login-node","title":"It is possible to SSH from the CE to the login node:","text":"<ul> <li>PATh staff know the current hostname of your login node</li> <li>That hostname has a public DNS entry that resolves to the correct IP address</li> <li>PATh staff know the user account name (default, \u201cosg01\u201d)</li> <li>PATh staff know about SSH configuration details to use (e.g., alternate port, jump host)</li> <li>The SSH client on one of our IP addresses can connect to your login node (through firewalls, etc.)</li> <li>The provided SSH public key has been installed in the right place and with the right permissions</li> <li>The provided SSH public key is sufficient for authentication by your SSH server</li> </ul>"},{"location":"ospool-requirements/#the-worker-nodes-on-which-our-jobs-may-run-are-ready","title":"The worker nodes on which our jobs may run are ready:","text":"<ul> <li>Our home directory is shared with each cluster node</li> <li>PATh staff know the correct path to scratch space for jobs (ideally on each worker node, but a shared filesystem may work)</li> <li>Our user account can create subdirectories and run executables in the scratch directory</li> <li>The worker nodes have permissive outbound network connectivity to the Internet (default allow, please note specific restrictions)</li> </ul>"}]}